/**
 * Minotaur Rust Code Generator
 *
 * High-performance Rust code generator with zero-cost abstractions,
 * ownership system integration, and modern Rust features.
 */

import { CodeGenerator, GeneratedCode, ContextAnalysisResult, ExportConfiguration } from '../CompilerCompilerExport';
import { Grammar } from '../../core/grammar/Grammar';

export class RustCodeGenerator extends CodeGenerator {
  private optimizationLevel: string = 'production';
  private rustEdition: string = '2021';
  private useAsyncAwait: boolean = false;

  public async generate(
    grammar: Grammar,
    contextInfo: ContextAnalysisResult,
    config: ExportConfiguration,
  ): Promise<GeneratedCode> {
    this.optimizationLevel = config.optimizationLevel;
    this.rustEdition = config.rust?.edition ?? '2021';
    this.useAsyncAwait = false; // Default value since not available in config,

    const sourceFiles = new Map<string, string>();
    const buildFiles = new Map<string, string>();

    const _grammarName = this.sanitizeIdentifier(grammar.getName() || 'grammar');

    // Generate Rust source files
    sourceFiles.set('lib.rs', this.generateLibModule(grammar, contextInfo, config));
    sourceFiles.set('parser.rs', this.generateParserModule(grammar, contextInfo, config));
    sourceFiles.set('lexer.rs', this.generateLexerModule(grammar, contextInfo, config));
    sourceFiles.set('ast.rs', this.generateASTModule(grammar, contextInfo, config));
    sourceFiles.set('token.rs', this.generateTokenModule(grammar, contextInfo, config));

    if (contextInfo.contextRequired) {
      sourceFiles.set('context.rs', this.generateContextModule(grammar, contextInfo, config));
    }

    // Generate build files
    if (config.buildSystemIntegration) {
      buildFiles.set('Cargo.toml', this.generateCargoToml(grammar, contextInfo, config));
      buildFiles.set('build.rs', this.generateBuildScript(grammar, contextInfo, config));
    }

    const linesOfCode = Array.from(sourceFiles.values()).reduce((total, content) =>
      total + content.split('\n').length, 0);

    return {
      sourceFiles,
      headerFiles: new Map(),
      buildFiles,
      documentationFiles: new Map(),
      testFiles: new Map(),
      metadata: {
        generationTime: Date.now(),
        linesOfCode,
        filesGenerated: sourceFiles.size + buildFiles.size,
        targetLanguage: 'Rust',
        embeddedLanguagesSupported: [],
        contextSwitchesGenerated: 0,
        crossLanguageReferencesGenerated: 0,
        validationRulesGenerated: 0,
        optimizationLevel: this.optimizationLevel,
        generatorVersion: '1.0.0',
        grammarComplexity: {
          nonTerminals: 0,
          terminals: 0,
          productions: 0,
          embeddedLanguages: 0,
          crossLanguageReferences: 0,
          validationRules: 0,
          cyclomaticComplexity: 1,
        },
      },
      success: true,
      errors: [],
      warnings: [],
    };
  }

  private generateLibModule(grammar: Grammar, contextInfo: ContextAnalysisResult, config: ExportConfiguration): string {
    const grammarName = this.sanitizeIdentifier(grammar.getName() || 'grammar');

    return `//! ${this.capitalizeFirst(grammarName)} Parser Library
//! 
//! High-performance parser generated by Minotaur Compiler-Compiler Export System
//! Optimized for zero-cost abstractions and memory safety

#![deny(unsafe_code)]
#![warn(missing_docs)]

pub mod parser;
pub mod lexer;
pub mod ast;
pub mod token;
${contextInfo.contextRequired ? 'pub mod context;' : ''}

pub use parser::{${this.capitalizeFirst(grammarName)}Parser, ParseError, ParseResult};
pub use lexer::{${this.capitalizeFirst(grammarName)}Lexer, LexError};
pub use ast::{ASTNode, ASTNodeType, ASTVisitor};
pub use token::{Token, TokenType};
${contextInfo.contextRequired ? `pub use context::{${this.capitalizeFirst(grammarName)}Context, Symbol, SymbolType, Scope};` : ''}

/// Parse a string input and return the AST
pub fn parse_string(input: &str) -> ParseResult<ASTNode> {
    let lexer = ${this.capitalizeFirst(grammarName)}Lexer::new(input);
    let mut parser = ${this.capitalizeFirst(grammarName)}Parser::new(lexer);
    parser.parse()
}

/// Parse a file and return the AST
pub fn parse_file(path: &std::path::Path) -> Result<ASTNode, Box<dyn std::error::Error>> {
    let content = std::fs::read_to_string(path)?;
    parse_string(&content).map_err(|e| e.into())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_parse_empty() {
        let result = parse_string("");
        assert!(result.is_ok());
    }

    #[test]
    fn test_parse_simple() {
        let result = parse_string("1 + 2");
        assert!(result.is_ok());
        let ast = result.unwrap();
        assert_eq!(ast.node_type(), ASTNodeType::Program);
    }
}
`;
  }

  // eslint-disable-next-line max-len
  private generateParserModule(grammar: Grammar, contextInfo: ContextAnalysisResult, config: ExportConfiguration): string {
    const grammarName = this.sanitizeIdentifier(grammar.getName() || 'grammar');

    return `//! High-performance parser with zero-cost abstractions
//! Generated by Minotaur Compiler-Compiler Export System

use crate::lexer::{${this.capitalizeFirst(grammarName)}Lexer, LexError};
use crate::ast::{ASTNode, ASTNodeType};
use crate::token::{Token, TokenType};
${contextInfo.contextRequired ? `use crate::context::${this.capitalizeFirst(grammarName)}Context;` : ''}
use std::fmt;

/// Parse result type alias
pub type ParseResult<T> = Result<T, ParseError>;

/// Parse error with detailed information
#[derive(Debug, Clone, PartialEq)]
pub struct ParseError {
    /// Error message
    pub message: String,
    /// Line number (1-based)
    pub line: usize,
    /// Column number (1-based)
    pub column: usize,
}

impl fmt::Display for ParseError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "Parse error at {}:{}: {}", self.line, self.column, self.message)
    }
}

impl std::error::Error for ParseError {}

impl From<LexError> for ParseError {
    fn from(err: LexError) -> Self {
        ParseError {
            message: err.message,
            line: err.line,
            column: err.column,
        }
    }
}

/// High-performance parser with zero-cost abstractions
pub struct ${this.capitalizeFirst(grammarName)}Parser<'input> {
    lexer: ${this.capitalizeFirst(grammarName)}Lexer<'input>,
    current_token: Option<Token>,
    lookahead_token: Option<Token>,
    errors: Vec<ParseError>,
    ${contextInfo.contextRequired ? `context: ${this.capitalizeFirst(grammarName)}Context,` : ''}
}

impl<'input> ${this.capitalizeFirst(grammarName)}Parser<'input> {
    /// Create a new parser with the given lexer
    pub fn new(mut lexer: ${this.capitalizeFirst(grammarName)}Lexer<'input>) -> Self {
        let current_token = lexer.next_token().ok();
        let lookahead_token = lexer.next_token().ok();
        
        Self {
            lexer,
            current_token,
            lookahead_token,
            errors: Vec::new(),
            ${contextInfo.contextRequired ? `context: ${this.capitalizeFirst(grammarName)}Context::new(),` : ''}
        }
    }

    /// Parse the input and return the AST root node
    pub fn parse(&mut self) -> ParseResult<ASTNode> {
        self.parse_start()
    }

    /// Get parse errors
    pub fn errors(&self) -> &[ParseError] {
        &self.errors
    }

    /// Advance to the next token
    fn advance(&mut self) -> Result<(), ParseError> {
        self.current_token = self.lookahead_token.take();
        self.lookahead_token = self.lexer.next_token().ok();
        Ok(())
    }

    /// Check if current token matches expected type and advance if so
    fn match_token(&mut self, expected: TokenType) -> bool {
        if let Some(ref token) = self.current_token {
            if token.token_type == expected {
                let _ = self.advance();
                return true;
            }
        }
        false
    }

    /// Expect a specific token type or return error
    fn expect(&mut self, expected: TokenType) -> ParseResult<Token> {
        if let Some(token) = &self.current_token {
            if token.token_type == expected {
                let result = token.clone();
                self.advance()?;
                Ok(result)
            } else {
                Err(ParseError {
                    message: format!("Expected {:?} but found {:?}", expected, token.token_type),
                    line: token.line,
                    column: token.column,
                })
            }
        } else {
            Err(ParseError {
                message: format!("Expected {:?} but reached end of input", expected),
                line: 0,
                column: 0,
            })
        }
    }

    // Production rule implementations

    /// Parse the start rule
    fn parse_start(&mut self) -> ParseResult<ASTNode> {
        let mut node = ASTNode::new(ASTNodeType::Program, None);

        while let Some(ref token) = self.current_token {
            if token.token_type == TokenType::EOF {
                break;
            }

            match self.parse_statement() {
                Ok(stmt) => node.add_child(stmt),
                Err(e) => {
                    self.errors.push(e);
                    // Error recovery: skip to next statement
                    self.skip_to_recovery_point();
                }
            }
        }

        Ok(node)
    }

    /// Parse an expression with operator precedence
    fn parse_expression(&mut self) -> ParseResult<ASTNode> {
        self.parse_binary_expression(0)
    }

    /// Parse binary expression with precedence climbing
    fn parse_binary_expression(&mut self, min_prec: u8) -> ParseResult<ASTNode> {
        let mut left = self.parse_primary()?;

        while let Some(ref token) = self.current_token {
            let prec = self.get_operator_precedence(token.token_type);
            if prec < min_prec {
                break;
            }

            let op_token = token.clone();
            self.advance()?;

            let right = self.parse_binary_expression(prec + 1)?;

            let mut op_node = ASTNode::new(ASTNodeType::BinaryOp, Some(op_token));
            op_node.add_child(left);
            op_node.add_child(right);
            left = op_node;
        }

        Ok(left)
    }

    /// Parse primary expression
    fn parse_primary(&mut self) -> ParseResult<ASTNode> {
        if let Some(ref token) = self.current_token {
            match token.token_type {
                TokenType::Number | TokenType::Identifier => {
                    let result = ASTNode::new(ASTNodeType::Terminal, Some(token.clone()));
                    self.advance()?;
                    Ok(result)
                }
                TokenType::LeftParen => {
                    self.advance()?; // consume '('
                    let expr = self.parse_expression()?;
                    self.expect(TokenType::RightParen)?;
                    Ok(expr)
                }
                _ => Err(ParseError {
                    message: format!("Unexpected token: {:?}", token.token_type),
                    line: token.line,
                    column: token.column,
                }),
            }
        } else {
            Err(ParseError {
                message: "Unexpected end of input".to_string(),
                line: 0,
                column: 0,
            })
        }
    }

    /// Parse a statement
    fn parse_statement(&mut self) -> ParseResult<ASTNode> {
        let mut node = ASTNode::new(ASTNodeType::Statement, None);
        
        let expr = self.parse_expression()?;
        node.add_child(expr);
        
        // Optional semicolon
        self.match_token(TokenType::Semicolon);
        
        Ok(node)
    }

    /// Get operator precedence
    fn get_operator_precedence(&self, token_type: TokenType) -> u8 {
        match token_type {
            TokenType::Multiply | TokenType::Divide => 20,
            TokenType::Plus | TokenType::Minus => 10,
            _ => 0,
        }
    }

    /// Skip tokens until a recovery point is found
    fn skip_to_recovery_point(&mut self) {
        while let Some(ref token) = self.current_token {
            match token.token_type {
                TokenType::Semicolon | TokenType::EOF => {
                    if token.token_type == TokenType::Semicolon {
                        let _ = self.advance();
                    }
                    break;
                }
                _ => {
                    let _ = self.advance();
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::lexer::${this.capitalizeFirst(grammarName)}Lexer;

    #[test]
    fn test_parse_number() {
        let lexer = ${this.capitalizeFirst(grammarName)}Lexer::new("42");
        let mut parser = ${this.capitalizeFirst(grammarName)}Parser::new(lexer);
        let result = parser.parse();
        assert!(result.is_ok());
    }

    #[test]
    fn test_parse_expression() {
        let lexer = ${this.capitalizeFirst(grammarName)}Lexer::new("1 + 2 * 3");
        let mut parser = ${this.capitalizeFirst(grammarName)}Parser::new(lexer);
        let result = parser.parse();
        assert!(result.is_ok());
    }

    #[test]
    fn test_parse_error() {
        let lexer = ${this.capitalizeFirst(grammarName)}Lexer::new("1 +");
        let mut parser = ${this.capitalizeFirst(grammarName)}Parser::new(lexer);
        let result = parser.parse();
        assert!(result.is_err());
    }
}
`;
  }

  // eslint-disable-next-line max-len
  private generateLexerModule(grammar: Grammar, contextInfo: ContextAnalysisResult, config: ExportConfiguration): string {
    const grammarName = this.sanitizeIdentifier(grammar.getName() || 'grammar');

    return `//! High-performance lexer with zero-cost abstractions
//! Generated by Minotaur Compiler-Compiler Export System

use crate::token::{Token, TokenType};
use std::fmt;

/// Lexical analysis error
#[derive(Debug, Clone, PartialEq)]
pub struct LexError {
    /// Error message
    pub message: String,
    /// Line number (1-based)
    pub line: usize,
    /// Column number (1-based)
    pub column: usize,
}

impl fmt::Display for LexError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "Lex error at {}:{}: {}", self.line, self.column, self.message)
    }
}

impl std::error::Error for LexError {}

/// High-performance lexer with zero-cost abstractions
pub struct ${this.capitalizeFirst(grammarName)}Lexer<'input> {
    input: &'input str,
    position: usize,
    line: usize,
    column: usize,
    chars: std::str::Chars<'input>,
    current_char: Option<char>,
}

impl<'input> ${this.capitalizeFirst(grammarName)}Lexer<'input> {
    /// Create a new lexer for the given input
    pub fn new(input: &'input str) -> Self {
        let mut chars = input.chars();
        let current_char = chars.next();
        
        Self {
            input,
            position: 0,
            line: 1,
            column: 1,
            chars,
            current_char,
        }
    }

    /// Get the next token from the input
    pub fn next_token(&mut self) -> Result<Token, LexError> {
        self.skip_whitespace();

        if self.current_char.is_none() {
            return Ok(Token::new(TokenType::EOF, "", self.line, self.column));
        }

        let start_line = self.line;
        let start_column = self.column;

        match self.current_char.unwrap() {
            // Numbers
            '0'..='9' => self.read_number(start_line, start_column),
            
            // Identifiers and keywords
            'a'..='z' | 'A'..='Z' | '_' => self.read_identifier(start_line, start_column),
            
            // String literals
            '"' => self.read_string(start_line, start_column),
            
            // Single-character tokens
            '+' => {
                self.advance();
                Ok(Token::new(TokenType::Plus, "+", start_line, start_column))
            }
            '-' => {
                self.advance();
                Ok(Token::new(TokenType::Minus, "-", start_line, start_column))
            }
            '*' => {
                self.advance();
                Ok(Token::new(TokenType::Multiply, "*", start_line, start_column))
            }
            '/' => {
                self.advance();
                Ok(Token::new(TokenType::Divide, "/", start_line, start_column))
            }
            '(' => {
                self.advance();
                Ok(Token::new(TokenType::LeftParen, "(", start_line, start_column))
            }
            ')' => {
                self.advance();
                Ok(Token::new(TokenType::RightParen, ")", start_line, start_column))
            }
            ';' => {
                self.advance();
                Ok(Token::new(TokenType::Semicolon, ";", start_line, start_column))
            }
            
            // Unknown character
            ch => {
                self.advance();
                Err(LexError {
                    message: format!("Unexpected character: '{}'", ch),
                    line: start_line,
                    column: start_column,
                })
            }
        }
    }

    /// Skip whitespace characters
    fn skip_whitespace(&mut self) {
        while let Some(ch) = self.current_char {
            if ch.is_whitespace() {
                self.advance();
            } else {
                break;
            }
        }
    }

    /// Advance to the next character
    fn advance(&mut self) {
        if let Some(ch) = self.current_char {
            self.position += ch.len_utf8();
            
            if ch == '\\n' {
                self.line += 1;
                self.column = 1;
            } else {
                self.column += 1;
            }
        }
        
        self.current_char = self.chars.next();
    }

    /// Read a number token
    fn read_number(&mut self, line: usize, column: usize) -> Result<Token, LexError> {
        let start_pos = self.position;
        
        // Read integer part
        while let Some(ch) = self.current_char {
            if ch.is_ascii_digit() {
                self.advance();
            } else {
                break;
            }
        }
        
        // Check for decimal point
        if self.current_char == Some('.') {
            self.advance();
            
            // Read fractional part
            while let Some(ch) = self.current_char {
                if ch.is_ascii_digit() {
                    self.advance();
                } else {
                    break;
                }
            }
        }
        
        let text = &self.input[start_pos..self.position];
        Ok(Token::new(TokenType::Number, text, line, column))
    }

    /// Read an identifier or keyword token
    fn read_identifier(&mut self, line: usize, column: usize) -> Result<Token, LexError> {
        let start_pos = self.position;
        
        while let Some(ch) = self.current_char {
            if ch.is_ascii_alphanumeric() || ch == '_' {
                self.advance();
            } else {
                break;
            }
        }
        
        let text = &self.input[start_pos..self.position];
        
        // Check for keywords
        let token_type = match text {
            // Add keywords based on grammar
            _ => TokenType::Identifier,
        };
        
        Ok(Token::new(token_type, text, line, column))
    }

    /// Read a string literal token
    fn read_string(&mut self, line: usize, column: usize) -> Result<Token, LexError> {
        let start_pos = self.position;
        self.advance(); // consume opening quote
        
        while let Some(ch) = self.current_char {
            if ch == '"' {
                self.advance(); // consume closing quote
                break;
            } else if ch == '\\\\' {
                self.advance(); // consume backslash
                if self.current_char.is_some() {
                    self.advance(); // consume escaped character
                }
            } else {
                self.advance();
            }
        }
        
        let text = &self.input[start_pos..self.position];
        Ok(Token::new(TokenType::String, text, line, column))
    }

    /// Get current position
    pub fn position(&self) -> usize {
        self.position
    }

    /// Get current line
    pub fn line(&self) -> usize {
        self.line
    }

    /// Get current column
    pub fn column(&self) -> usize {
        self.column
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_lex_number() {
        let mut lexer = ${this.capitalizeFirst(grammarName)}Lexer::new("42");
        let token = lexer.next_token().unwrap();
        assert_eq!(token.token_type, TokenType::Number);
        assert_eq!(token.text, "42");
    }

    #[test]
    fn test_lex_identifier() {
        let mut lexer = ${this.capitalizeFirst(grammarName)}Lexer::new("hello");
        let token = lexer.next_token().unwrap();
        assert_eq!(token.token_type, TokenType::Identifier);
        assert_eq!(token.text, "hello");
    }

    #[test]
    fn test_lex_operators() {
        let mut lexer = ${this.capitalizeFirst(grammarName)}Lexer::new("+ - * /");
        
        assert_eq!(lexer.next_token().unwrap().token_type, TokenType::Plus);
        assert_eq!(lexer.next_token().unwrap().token_type, TokenType::Minus);
        assert_eq!(lexer.next_token().unwrap().token_type, TokenType::Multiply);
        assert_eq!(lexer.next_token().unwrap().token_type, TokenType::Divide);
    }

    #[test]
    fn test_lex_string() {
        let mut lexer = ${this.capitalizeFirst(grammarName)}Lexer::new(r#""hello world""#);
        let token = lexer.next_token().unwrap();
        assert_eq!(token.token_type, TokenType::String);
        assert_eq!(token.text, r#""hello world""#);
    }
}
`;
  }

  private generateASTModule(grammar: Grammar, contextInfo: ContextAnalysisResult, config: ExportConfiguration): string {
    return `//! AST node types and visitor pattern implementation
//! Generated by Minotaur Compiler-Compiler Export System

use crate::token::Token;
use std::fmt;

/// AST node types
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum ASTNodeType {
    /// Terminal node (leaf)
    Terminal,
    /// Program root
    Program,
    /// Expression
    Expression,
    /// Binary operation
    BinaryOp,
    /// Statement
    Statement,
    /// Factor
    Factor,
    /// Term
    Term,
}

impl fmt::Display for ASTNodeType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            ASTNodeType::Terminal => write!(f, "Terminal"),
            ASTNodeType::Program => write!(f, "Program"),
            ASTNodeType::Expression => write!(f, "Expression"),
            ASTNodeType::BinaryOp => write!(f, "BinaryOp"),
            ASTNodeType::Statement => write!(f, "Statement"),
            ASTNodeType::Factor => write!(f, "Factor"),
            ASTNodeType::Term => write!(f, "Term"),
        }
    }
}

/// AST node with zero-cost abstractions
#[derive(Debug, Clone)]
pub struct ASTNode {
    node_type: ASTNodeType,
    token: Option<Token>,
    children: Vec<ASTNode>,
}

impl ASTNode {
    /// Create a new AST node
    pub fn new(node_type: ASTNodeType, token: Option<Token>) -> Self {
        Self {
            node_type,
            token,
            children: Vec::new(),
        }
    }

    /// Get the node type
    pub fn node_type(&self) -> ASTNodeType {
        self.node_type
    }

    /// Get the token (for terminal nodes)
    pub fn token(&self) -> Option<&Token> {
        self.token.as_ref()
    }

    /// Get child nodes
    pub fn children(&self) -> &[ASTNode] {
        &self.children
    }

    /// Get mutable child nodes
    pub fn children_mut(&mut self) -> &mut Vec<ASTNode> {
        &mut self.children
    }

    /// Add a child node
    pub fn add_child(&mut self, child: ASTNode) {
        self.children.push(child);
    }

    /// Remove a child node at the given index
    pub fn remove_child(&mut self, index: usize) -> Option<ASTNode> {
        if index < self.children.len() {
            Some(self.children.remove(index))
        } else {
            None
        }
    }

    /// Check if this is a terminal node
    pub fn is_terminal(&self) -> bool {
        self.token.is_some()
    }

    /// Check if this is a leaf node
    pub fn is_leaf(&self) -> bool {
        self.children.is_empty()
    }

    /// Accept a visitor (visitor pattern)
    pub fn accept<V: ASTVisitor>(&self, visitor: &mut V) {
        visitor.visit(self);
    }

    /// Traverse the tree in pre-order
    pub fn traverse_preorder<F>(&self, visitor: &mut F)
    where
        F: FnMut(&ASTNode),
    {
        visitor(self);
        for child in &self.children {
            child.traverse_preorder(visitor);
        }
    }

    /// Traverse the tree in post-order
    pub fn traverse_postorder<F>(&self, visitor: &mut F)
    where
        F: FnMut(&ASTNode),
    {
        for child in &self.children {
            child.traverse_postorder(visitor);
        }
        visitor(self);
    }

    /// Find the first node of the specified type
    pub fn find_first(&self, node_type: ASTNodeType) -> Option<&ASTNode> {
        if self.node_type == node_type {
            return Some(self);
        }

        for child in &self.children {
            if let Some(found) = child.find_first(node_type) {
                return Some(found);
            }
        }

        None
    }

    /// Find all nodes of the specified type
    pub fn find_all(&self, node_type: ASTNodeType) -> Vec<&ASTNode> {
        let mut result = Vec::new();
        self.find_all_recursive(node_type, &mut result);
        result
    }

    /// Recursive helper for find_all
    fn find_all_recursive(&self, node_type: ASTNodeType, result: &mut Vec<&ASTNode>) {
        if self.node_type == node_type {
            result.push(self);
        }

        for child in &self.children {
            child.find_all_recursive(node_type, result);
        }
    }

    /// Get the depth of the tree
    pub fn depth(&self) -> usize {
        if self.children.is_empty() {
            1
        } else {
            1 + self.children.iter().map(|child| child.depth()).max().unwrap_or(0)
        }
    }

    /// Count the total number of nodes in the tree
    pub fn node_count(&self) -> usize {
        1 + self.children.iter().map(|child| child.node_count()).sum::<usize>()
    }
}

impl fmt::Display for ASTNode {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        if let Some(token) = &self.token {
            write!(f, "{}: {:?}", self.node_type, token.text)
        } else {
            write!(f, "{}", self.node_type)
        }
    }
}

/// Visitor trait for AST traversal
pub trait ASTVisitor {
    /// Visit a node
    fn visit(&mut self, node: &ASTNode);
}

/// Print visitor for debugging
pub struct PrintVisitor {
    indent: usize,
}

impl PrintVisitor {
    /// Create a new print visitor
    pub fn new() -> Self {
        Self { indent: 0 }
    }
}

impl Default for PrintVisitor {
    fn default() -> Self {
        Self::new()
    }
}

impl ASTVisitor for PrintVisitor {
    fn visit(&mut self, node: &ASTNode) {
        println!("{}{}", "  ".repeat(self.indent), node);
        
        self.indent += 1;
        for child in node.children() {
            child.accept(self);
        }
        self.indent -= 1;
    }
}

/// Print the AST in a readable format
pub fn print_ast(node: &ASTNode) {
    let mut visitor = PrintVisitor::new();
    node.accept(&mut visitor);
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::token::{Token, TokenType};

    #[test]
    fn test_ast_node_creation() {
        let token = Token::new(TokenType::Number, "42", 1, 1);
        let node = ASTNode::new(ASTNodeType::Terminal, Some(token));
        
        assert_eq!(node.node_type(), ASTNodeType::Terminal);
        assert!(node.is_terminal());
        assert!(node.is_leaf());
    }

    #[test]
    fn test_ast_node_children() {
        let mut parent = ASTNode::new(ASTNodeType::Program, None);
        let child1 = ASTNode::new(ASTNodeType::Statement, None);
        let child2 = ASTNode::new(ASTNodeType::Expression, None);
        
        parent.add_child(child1);
        parent.add_child(child2);
        
        assert_eq!(parent.children().len(), 2);
        assert!(!parent.is_leaf());
    }

    #[test]
    fn test_find_nodes() {
        let mut root = ASTNode::new(ASTNodeType::Program, None);
        let mut stmt = ASTNode::new(ASTNodeType::Statement, None);
        let expr = ASTNode::new(ASTNodeType::Expression, None);
        
        stmt.add_child(expr);
        root.add_child(stmt);
        
        let found = root.find_first(ASTNodeType::Expression);
        assert!(found.is_some());
        
        let all_found = root.find_all(ASTNodeType::Expression);
        assert_eq!(all_found.len(), 1);
    }
}
`;
  }

  // eslint-disable-next-line max-len
  private generateTokenModule(grammar: Grammar, contextInfo: ContextAnalysisResult, config: ExportConfiguration): string {
    return `//! Token types and definitions
//! Generated by Minotaur Compiler-Compiler Export System

use std::fmt;

/// Token types for lexical analysis
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum TokenType {
    /// End of file
    EOF,
    /// Lexical error
    Error,
    /// Identifier
    Identifier,
    /// Number literal
    Number,
    /// String literal
    String,
    /// Plus operator (+)
    Plus,
    /// Minus operator (-)
    Minus,
    /// Multiply operator (*)
    Multiply,
    /// Divide operator (/)
    Divide,
    /// Left parenthesis (()
    LeftParen,
    /// Right parenthesis ())
    RightParen,
    /// Semicolon (;)
    Semicolon,
    // Add more tokens based on grammar
}

impl fmt::Display for TokenType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            TokenType::EOF => write!(f, "EOF"),
            TokenType::Error => write!(f, "ERROR"),
            TokenType::Identifier => write!(f, "IDENTIFIER"),
            TokenType::Number => write!(f, "NUMBER"),
            TokenType::String => write!(f, "STRING"),
            TokenType::Plus => write!(f, "+"),
            TokenType::Minus => write!(f, "-"),
            TokenType::Multiply => write!(f, "*"),
            TokenType::Divide => write!(f, "/"),
            TokenType::LeftParen => write!(f, "("),
            TokenType::RightParen => write!(f, ")"),
            TokenType::Semicolon => write!(f, ";"),
        }
    }
}

/// Token with position information
#[derive(Debug, Clone, PartialEq)]
pub struct Token {
    /// Token type
    pub token_type: TokenType,
    /// Token text
    pub text: String,
    /// Line number (1-based)
    pub line: usize,
    /// Column number (1-based)
    pub column: usize,
}

impl Token {
    /// Create a new token
    pub fn new(token_type: TokenType, text: &str, line: usize, column: usize) -> Self {
        Self {
            token_type,
            text: text.to_string(),
            line,
            column,
        }
    }

    /// Get token type
    pub fn token_type(&self) -> TokenType {
        self.token_type
    }

    /// Get token text
    pub fn text(&self) -> &str {
        &self.text
    }

    /// Get line number
    pub fn line(&self) -> usize {
        self.line
    }

    /// Get column number
    pub fn column(&self) -> usize {
        self.column
    }

    /// Check if this is an EOF token
    pub fn is_eof(&self) -> bool {
        self.token_type == TokenType::EOF
    }

    /// Check if this is an error token
    pub fn is_error(&self) -> bool {
        self.token_type == TokenType::Error
    }
}

impl fmt::Display for Token {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{} '{}' at {}:{}", self.token_type, self.text, self.line, self.column)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_token_creation() {
        let token = Token::new(TokenType::Number, "42", 1, 1);
        assert_eq!(token.token_type(), TokenType::Number);
        assert_eq!(token.text(), "42");
        assert_eq!(token.line(), 1);
        assert_eq!(token.column(), 1);
    }

    #[test]
    fn test_token_display() {
        let token = Token::new(TokenType::Plus, "+", 1, 5);
        let display = format!("{}", token);
        assert!(display.contains("+"));
        assert!(display.contains("1:5"));
    }

    #[test]
    fn test_token_type_display() {
        assert_eq!(format!("{}", TokenType::Plus), "+");
        assert_eq!(format!("{}", TokenType::Identifier), "IDENTIFIER");
    }
}
`;
  }

  // eslint-disable-next-line max-len
  private generateContextModule(grammar: Grammar, contextInfo: ContextAnalysisResult, config: ExportConfiguration): string {
    const grammarName = this.sanitizeIdentifier(grammar.getName() || 'grammar');

    return `//! Context management for scope and symbol tracking
//! Generated by Minotaur Compiler-Compiler Export System

use std::collections::HashMap;
use std::fmt;

/// Symbol types for context management
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum SymbolType {
    /// Variable symbol
    Variable,
    /// Function symbol
    Function,
    /// Class symbol
    Class,
    /// Interface symbol
    Interface,
}

impl fmt::Display for SymbolType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            SymbolType::Variable => write!(f, "Variable"),
            SymbolType::Function => write!(f, "Function"),
            SymbolType::Class => write!(f, "Class"),
            SymbolType::Interface => write!(f, "Interface"),
        }
    }
}

/// Symbol with metadata
#[derive(Debug, Clone)]
pub struct Symbol {
    name: String,
    symbol_type: SymbolType,
    data: Option<String>, // Additional data as string for simplicity
}

impl Symbol {
    /// Create a new symbol
    pub fn new(name: String, symbol_type: SymbolType, data: Option<String>) -> Self {
        Self {
            name,
            symbol_type,
            data,
        }
    }

    /// Get symbol name
    pub fn name(&self) -> &str {
        &self.name
    }

    /// Get symbol type
    pub fn symbol_type(&self) -> SymbolType {
        self.symbol_type
    }

    /// Get symbol data
    pub fn data(&self) -> Option<&str> {
        self.data.as_deref()
    }
}

impl fmt::Display for Symbol {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{}: {}", self.symbol_type, self.name)
    }
}

/// Scope for symbol management
#[derive(Debug, Clone)]
pub struct Scope {
    name: String,
    symbols: HashMap<String, Symbol>,
}

impl Scope {
    /// Create a new scope
    pub fn new(name: String) -> Self {
        Self {
            name,
            symbols: HashMap::new(),
        }
    }

    /// Get scope name
    pub fn name(&self) -> &str {
        &self.name
    }

    /// Define a symbol in this scope
    pub fn define_symbol(&mut self, name: String, symbol_type: SymbolType, data: Option<String>) -> bool {
        if self.symbols.contains_key(&name) {
            false // Symbol already exists
        } else {
            self.symbols.insert(name.clone(), Symbol::new(name, symbol_type, data));
            true
        }
    }

    /// Look up a symbol in this scope
    pub fn lookup_symbol(&self, name: &str) -> Option<&Symbol> {
        self.symbols.get(name)
    }

    /// Check if symbol exists in this scope
    pub fn has_symbol(&self, name: &str) -> bool {
        self.symbols.contains_key(name)
    }

    /// Get all symbols in this scope
    pub fn symbols(&self) -> impl Iterator<Item = &Symbol> {
        self.symbols.values()
    }

    /// Get symbol count
    pub fn symbol_count(&self) -> usize {
        self.symbols.len()
    }
}

impl fmt::Display for Scope {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "Scope '{}' ({} symbols)", self.name, self.symbols.len())
    }
}

/// Context management for scope and symbol tracking
#[derive(Debug)]
pub struct ${this.capitalizeFirst(grammarName)}Context {
    scope_stack: Vec<Scope>,
}

impl ${this.capitalizeFirst(grammarName)}Context {
    /// Create a new context with global scope
    pub fn new() -> Self {
        let mut context = Self {
            scope_stack: Vec::new(),
        };
        context.scope_stack.push(Scope::new("global".to_string()));
        context
    }

    /// Push a new scope onto the stack
    pub fn push_scope(&mut self, name: String) {
        self.scope_stack.push(Scope::new(name));
    }

    /// Pop the current scope from the stack
    pub fn pop_scope(&mut self) -> Option<Scope> {
        if self.scope_stack.len() > 1 { // Keep global scope
            self.scope_stack.pop()
        } else {
            None
        }
    }

    /// Get the current scope
    pub fn current_scope(&self) -> &Scope {
        self.scope_stack.last().unwrap() // Always has at least global scope
    }

    /// Get mutable reference to current scope
    pub fn current_scope_mut(&mut self) -> &mut Scope {
        self.scope_stack.last_mut().unwrap() // Always has at least global scope
    }

    /// Get the global scope
    pub fn global_scope(&self) -> &Scope {
        &self.scope_stack[0]
    }

    /// Define a symbol in the current scope
    pub fn define_symbol(&mut self, name: String, symbol_type: SymbolType, data: Option<String>) -> bool {
        self.current_scope_mut().define_symbol(name, symbol_type, data)
    }

    /// Look up a symbol starting from current scope
    pub fn lookup_symbol(&self, name: &str) -> Option<&Symbol> {
        // Search from current scope up to global
        for scope in self.scope_stack.iter().rev() {
            if let Some(symbol) = scope.lookup_symbol(name) {
                return Some(symbol);
            }
        }
        None
    }

    /// Check if a symbol is defined in any scope
    pub fn is_symbol_defined(&self, name: &str) -> bool {
        self.lookup_symbol(name).is_some()
    }

    /// Get the current scope depth
    pub fn scope_depth(&self) -> usize {
        self.scope_stack.len() - 1 // Exclude global scope
    }

    /// Get names of all scopes in the stack
    pub fn scope_names(&self) -> Vec<&str> {
        self.scope_stack.iter().map(|scope| scope.name()).collect()
    }

    /// Get total symbol count across all scopes
    pub fn total_symbol_count(&self) -> usize {
        self.scope_stack.iter().map(|scope| scope.symbol_count()).sum()
    }
}

impl Default for ${this.capitalizeFirst(grammarName)}Context {
    fn default() -> Self {
        Self::new()
    }
}

impl fmt::Display for ${this.capitalizeFirst(grammarName)}Context {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "Context (depth: {}, scopes: {:?})", 
               self.scope_depth(), 
               self.scope_names())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_symbol_creation() {
        let symbol = Symbol::new("x".to_string(), SymbolType::Variable, None);
        assert_eq!(symbol.name(), "x");
        assert_eq!(symbol.symbol_type(), SymbolType::Variable);
        assert!(symbol.data().is_none());
    }

    #[test]
    fn test_scope_operations() {
        let mut scope = Scope::new("test".to_string());
        assert_eq!(scope.name(), "test");
        assert_eq!(scope.symbol_count(), 0);

        let success = scope.define_symbol("x".to_string(), SymbolType::Variable, None);
        assert!(success);
        assert_eq!(scope.symbol_count(), 1);

        let duplicate = scope.define_symbol("x".to_string(), SymbolType::Function, None);
        assert!(!duplicate); // Should fail due to duplicate name

        let symbol = scope.lookup_symbol("x");
        assert!(symbol.is_some());
        assert_eq!(symbol.unwrap().symbol_type(), SymbolType::Variable);
    }

    #[test]
    fn test_context_operations() {
        let mut context = ${this.capitalizeFirst(grammarName)}Context::new();
        assert_eq!(context.scope_depth(), 0);

        context.push_scope("function".to_string());
        assert_eq!(context.scope_depth(), 1);

        let success = context.define_symbol("x".to_string(), SymbolType::Variable, None);
        assert!(success);

        let symbol = context.lookup_symbol("x");
        assert!(symbol.is_some());

        context.pop_scope();
        assert_eq!(context.scope_depth(), 0);

        // Symbol should no longer be found
        let symbol = context.lookup_symbol("x");
        assert!(symbol.is_none());
    }
}
`;
  }

  private generateCargoToml(grammar: Grammar, contextInfo: ContextAnalysisResult, config: ExportConfiguration): string {
    const grammarName = this.sanitizeIdentifier(grammar.getName() || 'grammar');

    return `[package]
name = "${grammarName}-parser"
version = "0.1.0"
edition = "${this.rustEdition}"
authors = ["Minotaur"]
description = "Generated parser for ${grammarName} grammar"
license = "MIT"
repository = "https://github.com/minotaur/${grammarName}-parser"
keywords = ["parser", "lexer", "grammar", "ast", "compiler"]
categories = ["parsing", "text-processing"]

[dependencies]
# No external dependencies - zero-cost abstractions only

[dev-dependencies]
criterion = "0.4"

[lib]
name = "${grammarName}_parser"
path = "src/lib.rs"

[[bench]]
name = "parser_benchmark"
harness = false

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
panic = "abort"

[profile.bench]
opt-level = 3
lto = true
codegen-units = 1
`;
  }

  // eslint-disable-next-line max-len
  private generateBuildScript(grammar: Grammar, contextInfo: ContextAnalysisResult, config: ExportConfiguration): string {
    return `// Build script for additional optimizations
// Generated by Minotaur Compiler-Compiler Export System

fn main() {
    // Enable additional optimizations for release builds
    if cfg!(not(debug_assertions)) {
        println!("cargo:rustc-link-arg=-s"); // Strip symbols
    }
    
    // Set target CPU for maximum performance
    println!("cargo:rustc-env=RUSTFLAGS=-C target-cpu=native");
    
    println!("cargo:rerun-if-changed=build.rs");
}
`;
  }

  protected sanitizeIdentifier(name: string): string {
    return name.replace(/[^a-zA-Z0-9_]/g, '_').toLowerCase();
  }

  private capitalizeFirst(str: string): string {
    return str.charAt(0).toUpperCase() + str.slice(1);
  }

  private getAppliedOptimizations(): string[] {
    return [
      'Zero-cost abstractions with compile-time optimization',
      'Ownership system for memory safety without GC overhead',
      'LLVM backend optimization with target-cpu=native',
      'Link-time optimization (LTO) for maximum performance',
      'Panic=abort for reduced binary size',
      'Iterator chains for efficient data processing',
      'Pattern matching for branch prediction optimization',
    ];
  }

  private getDependencies(config: ExportConfiguration): string[] {
    return [`Rust ${this.rustEdition} edition`, 'No external runtime dependencies'];
  }

  private getBuildInstructions(config: ExportConfiguration): string {
    return 'cargo build --release';
  }

  private getPerformanceNotes(): string[] {
    return [
      'Zero-cost abstractions - no runtime overhead',
      'LLVM optimization with native CPU targeting',
      'Memory safety without garbage collection',
      'Compile-time optimizations with monomorphization',
      'Efficient pattern matching with exhaustiveness checking',
      'Iterator fusion for optimal loop performance',
    ];
  }

  protected generateContextSwitchingCode(): string {
    return `// Context switching infrastructure for Rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

pub struct ContextManager {
    contexts: HashMap<String, Context>,
    current_context: Option<String>,
}

impl ContextManager {
    pub fn new() -> Self {
        Self {
            contexts: HashMap::new(),
            current_context: None,
        }
    }
    
    pub fn switch_context(&mut self, context_name: &str) -> Result<(), ContextError> {
        if self.contexts.contains_key(context_name) {
            self.current_context = Some(context_name.to_string());
            Ok(())
        } else {
            Err(ContextError::ContextNotFound(context_name.to_string()))
        }
    }
    
    pub fn get_current_context(&self) -> Option<&Context> {
        self.current_context.as_ref()
            .and_then(|name| self.contexts.get(name))
    }
}

#[derive(Debug)]
pub enum ContextError {
    ContextNotFound(String),
}`;
  }

  protected generateCrossLanguageValidationCode(): string {
    return '// Cross-language validation code for Rust (stub implementation)';
  }

  protected generateSymbolTableSharingCode(): string {
    return `// Symbol table sharing infrastructure for Rust
use std::collections::HashMap;
use std::sync::{Arc, RwLock};

pub type SymbolTable = HashMap<String, Symbol>;

#[derive(Clone, Debug)]
pub struct Symbol {
    pub name: String,
    pub symbol_type: SymbolType,
    pub scope: String,
    pub value: Option<String>,
}

#[derive(Clone, Debug)]
pub enum SymbolType {
    Variable,
    Function,
    Type,
    Constant,
}

pub struct SharedSymbolTable {
    table: Arc<RwLock<SymbolTable>>,
}

impl SharedSymbolTable {
    pub fn new() -> Self {
        Self {
            table: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    pub fn insert(&self, name: String, symbol: Symbol) -> Result<(), String> {
        match self.table.write() {
            Ok(mut table) => {
                table.insert(name, symbol);
                Ok(())
            }
            Err(_) => Err("Failed to acquire write lock".to_string()),
        }
    }
    
    pub fn get(&self, name: &str) -> Result<Option<Symbol>, String> {
        match self.table.read() {
            Ok(table) => Ok(table.get(name).cloned()),
            Err(_) => Err("Failed to acquire read lock".to_string()),
        }
    }
}`;
  }

  protected generateEmbeddedLanguageParser(language: string, grammar: Grammar): string {
    return `// Embedded language parser for ${language} in Rust (stub implementation)`;
  }

  protected generateEmbeddedLanguageParserTests(language: string, grammar: Grammar): string {
    return `// Embedded language parser tests for ${language} in Rust (stub implementation)`;
  }

  protected generateContextSwitchingTests(): string {
    return '// Context switching tests for Rust (stub implementation)';
  }

  protected generateCrossLanguageValidationTests(): string {
    return '// Cross-language validation tests for Rust (stub implementation)';
  }
}

